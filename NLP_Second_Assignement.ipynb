{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG8L18ZAk5zVbtrfw8xFNp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kidus-Bellete/NLP_Second_Assignment/blob/main/NLP_Second_Assignement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6NaAYu9qrqe",
        "outputId": "ce08c919-d279-45aa-89df-49cce76f4850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADV, ADJ\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def fetch_wikipedia_article(topic):\n",
        "    url = f\"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'format': 'json',\n",
        "        'titles': topic,\n",
        "        'prop': 'extracts',\n",
        "        'explaintext': True\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        page = next(iter(data['query']['pages'].values()))\n",
        "        return page.get('extract', \"No content available\")\n",
        "    else:\n",
        "        return \"Failed to fetch data\"\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return ADV\n",
        "    else:\n",
        "        return NOUN\n",
        "\n",
        "def preprocess(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    lemmatized = [lemmatizer.lemmatize(w.lower(), get_wordnet_pos(pos)) for w, pos in tagged if w.isalpha() and w.lower() not in stop_words]\n",
        "\n",
        "    deduplicated = []\n",
        "    for word in lemmatized:\n",
        "        if not deduplicated or word != deduplicated[-1]:\n",
        "            deduplicated.append(word)\n",
        "\n",
        "    return deduplicated\n",
        "\n",
        "def create_slices(text, slice_size, overlap):\n",
        "    words = preprocess(text)\n",
        "    slices = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = min(start + slice_size, len(words))\n",
        "        slices.append(' '.join(words[start:end]))\n",
        "        start += slice_size - overlap\n",
        "    return slices\n",
        "\n",
        "def compute_cosine_similarity(slices):\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectors = vectorizer.fit_transform(slices).toarray()\n",
        "    similarities = [cosine_similarity([vectors[i]], [vectors[i + 1]])[0][0] for i in range(len(vectors) - 1)]\n",
        "    return similarities\n",
        "\n",
        "def process_text_for_llm(text, slice_size, overlap, threshold):\n",
        "    slices = create_slices(text, slice_size, overlap)\n",
        "    if len(slices) < 2:\n",
        "        return slices\n",
        "\n",
        "    similarities = compute_cosine_similarity(slices)\n",
        "    distinct_slices = [slices[0]]\n",
        "\n",
        "    for i, similarity in enumerate(similarities):\n",
        "        if similarity < threshold:\n",
        "            distinct_slices.append(slices[i + 1])\n",
        "\n",
        "    return distinct_slices\n",
        "\n",
        "\n",
        "# topics both medical and non medical from previous assignemnts\n",
        "topics = [\"Science\", \"Experimental Science\", \"Theoretical Science\", \"Earth\", \"Ecosystem\", \"Python\",\n",
        "          \"Computer\", \"Health\", \"Philosophy\", \"Animal\", \"Nature\", \"History\",\"stomach\", \"Therapy\",\n",
        "          \"World\", \"Food\", \"Human\", \"Culture\", \"Italy\", \"Technology\", \"Natural Science\",\n",
        "          \"Medicine\", \"Hospital\", \"Surgery\", \"Health\", \"Heart\", \"Vaccine\", \"endurance\", \"brain\",\n",
        "          \"Pharmacy\", \"Immunology\", \"Pathology\", \"Treatment\", \"Diabetes\", \"Disease\",\n",
        "          \"Therapy\", \"Dentistry\", \"Kidney\", \"Blood\", \"Blood pressure\", \"Virus\",\n",
        "          \"Art\", \"Language\", \"Literature\", \"Political Science\", \"Theoretical Science\",\n",
        "          \"Empire\", \"Space\", \"Environment\", \"Color\", \"Mountain\", \"rule of law\", \"justice\",\n",
        "          \"Forest\", \"Cooking\", \"Theology\", \"Fashion\", \"animal\", \"love\", \"tree\",\n",
        "          \"History\", \"Geography\", \"Archaeology\", \"government\", \"Astronomy\"\n",
        "          ]\n",
        " # windw size is 128mb\n",
        "slice_size = 128 * 1024 * 1024   # window size\n",
        "overlap = 50\n",
        "threshold = 0.8\n",
        "\n",
        "#articles fetched from wikepedia\n",
        "all_articles = \"\"\n",
        "for topic in topics:\n",
        "    article = fetch_wikipedia_article(topic)\n",
        "    preprocessed_article = ' '.join(preprocess(article))\n",
        "    all_articles += preprocessed_article + \" \"\n",
        "\n",
        "\n",
        "# Process the combined text for LLM\n",
        "processed_slices = process_text_for_llm(all_articles, slice_size, overlap, threshold)\n",
        "\n",
        "# Save slices to a text file\n",
        "output_file_path = \"slices_output.txt\"\n",
        "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    for i, slice in enumerate(processed_slices):\n",
        "        output_file.write(f\"Slice {i+1} Length: {len(slice)}\\n\\n\")\n",
        "        output_file.write(slice + \"\\n\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "print(f\"The slices saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk_GN_c1qr18",
        "outputId": "97e51b08-058d-444d-f262-3509d9c2f90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slices saved to slices_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Function to load the GPT-2 model and tokenizer\n",
        "def load_model(model_name=\"gpt2-large\"):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Function to generate text from the GPT-2 model\n",
        "def generate_combined_text(prompt, model, tokenizer, max_length=300):\n",
        "    generated_texts = []\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate output based on the input prompt\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.9,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            no_repeat_ngram_size=2,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the generated output\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Split into sentences and adjust if needed\n",
        "    sentences = generated_text.split('.')\n",
        "    if len(sentences) > 1 and len(sentences[-1]) < 15:\n",
        "        generated_text = '.'.join(sentences[:-1]) + '.'\n",
        "\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "model, tokenizer = load_model(\"gpt2-large\")\n",
        "\n",
        "# Combine all processed slices into a single string\n",
        "combined_text = ' '.join(processed_slices)\n",
        "\n",
        "# Ensure the combined text doesn't exceed the maximum token limit\n",
        "max_token_limit = 1024\n",
        "if tokenizer.encode(combined_text, return_tensors=\"pt\").size(1) > max_token_limit:\n",
        "\n",
        "    combined_text = combined_text[:max_token_limit] # split the text as needed\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"What is the usage of science in our physical world?\"\n",
        "\n",
        "# Generate a single output from the combined text\n",
        "generated_text = generate_combined_text(prompt, model, tokenizer, max_length=300)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIOL17LfqsA0",
        "outputId": "56b4efed-425d-4e9a-dd38-3904adcc1449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (244267 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the usage of science in our physical world?\n",
            "\n",
            "Science is a method of discovering the laws of nature. It is not a science of the physical universe.\n",
            "The laws that govern the universe are not laws in the sense that they are immutable. They are laws which can be changed. The laws are the result of a process of evolution. Evolution is an ongoing process. There is no such thing as a law of physics. All the fundamental laws, such as gravity, are determined by the process that has led to the existence of matter. In other words, the law that governs the motion of an object is determined not by a fixed set of laws but by an evolutionary process which has produced the object. This process is called evolution, and it is what gives rise to all the other laws. For example, gravity is caused by gravity. Gravity is also caused when an electron is accelerated by its own mass. If the electron were to accelerate to a higher velocity, it would be able to escape from the gravitational field of its home planet. But if the mass of this electron was to be reduced, then the force of gravity would no longer be sufficient to keep it in orbit around the planet, so it could not escape. So the electrons would have to go somewhere else. And so on. Eventually, all of these laws would come to rest. As the evolution of all these objects continues, they would eventually come into equilibrium. At that point, there would not be any more laws\n"
          ]
        }
      ]
    }
  ]
}